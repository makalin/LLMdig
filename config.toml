[server]
host = "0.0.0.0"
port = 9000
max_connections = 1000
timeout_seconds = 30

[llm]
backend = "openai"
model = "gpt-3.5-turbo"
max_tokens = 256
temperature = 0.7
timeout_seconds = 30

[rate_limit]
enabled = true
requests_per_minute = 60
burst_size = 10 